Class: Data Mining 
Name: William Horn
Date: Sep 14, 2017 

Inferring Rudimentary Rules:
    One Rule Approach (1R):
        - Make one level decision based on tree based one attribute
        - given a training set
        - good way of handling missing values

        Note:
            - This is done for each attribute
            - the attribute with the least amount of errors
              is the one that is picked.
            - gives a pretty good results 

        - Even if results are bad, can still give insight
        - possibly notice a two level would be benifical

        Notes:
            - Do the simple stuff first (1R) befor moving
              onto more complex/costly methods

Numeric Attributes:
    - How to decide on how to branch on nominal values

    Approach:
        - Sort numbers
        - create break point in the number every time the classification changes
        - this partitions the domain

Overfitting:
    - too many partitions/breakpoints
    - too many breakpoints is counter productive

    Def: 
        - instead of categorizing things, fragmenting them.
        - instances are being overfit to existing classification in the 
          dataset

    Note:
        - can prevent predictions
        - each bucket is too small to gourp other data
          into it.
        - derived model is too specific to the training set
          to make predictions with new data

    Solving Overfitting:
        - specifiy that a larger, minimum number of instances from
          the training sets that have to be in each partition

        - if the majority from two ranges has the same majority
          the two partition should be merged.


Statistical Modeling:
    Bayes' Theorem Application: 
        - telling something conditional probability
        
    Niave Bays: 
        - Assumptions that attributes are indepenent
        - even if they are known to not be    
        
        Simplifying Assumptions:
            known falcities that are used to make calculations simpler.

            Given that it was sunny, what is the prob that I played the game 
                        <=> Gets transformed using bay's <=>
            Given that I played, what was the chance that I played.

        Bays': P(H|E) the prob of the hypo is true given some information
            Theorem: 
                P(H|E) = (P(E|H) * P(H)) / P(E)

            Application:
                H = (play = yes)
                E = (outlook = sunny)

                P(play = yes | outlook = sunny) = (P(outlook = sunny | play = yes) * P(play = yes)) / P(outlook = sunny)

        Note: 
            - composite expression can be done using bays for disjoint events
            - Just the product product rule

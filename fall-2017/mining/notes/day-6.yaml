Class: Data Mining 
Name: William Horn
Date: Oct 05, 2017

note: not on midterm


Chapter 5:
    - comparing different algorithms

Params for Comparison:
    - predicting classification
    - or prob that an istance is a particular class
    - numeric prediction

    Detecting Failer in Machine learning:
        - false positives/negatives
        
        note: these will have different costs in practice.
            - in stats type 1/2

    Cross Validation:
        - comparing data mining performance

Training and Testing:
    - how to estimate error rate? (num of incorrect predictions)

    - derived predictions structes may be imperfect.
    - these structs are derived from training set.
    - training set may not be optimized.

    Resubstittion Error:
        - run algo on data set it was trained on.
        - this has some value, but not the "true" error rate.

    Overfitting:
        - fiting model too closely to the training set.
        - optimized for training set not overal population.
        - tightly linked.

        note: on bigger training sets, less of a deal

    Error Rate: 
        - error rate is derived from the test set.
        - training and test set need to be independent
        - both test and training should be representive of the 
          whole population.

    Validation Set:
        - select one of many dm algos
        - also an set independent
        - optimized the selected algo


    Process:
        - train algo on training set
        - optimized algo on the test set
        - compare algos on the validation set

        note: all these sets need to be independent

Performance Perdiction:
    - determing success rates...        

Cross Validation:
    Random Sampling Problems:
        - if all/most of one class going into test set
        - then training set won't know any information about
          classifying this situation.
        - this is a problem with fully random samples

        solution:
            Stratification:
                - random samples are done on a class by class.
                - 2/3 of each class goes into training set
                  1/3 into test set

        Repeated Holdout:
            - improving estimate on error rate.
            - n times, randomly hold out 1/3 of instances for training.
            - maybe w/ stratification

        idea: 
            - sometimes flip whether the training or test gets which portion of the data
            - this leads to being able to get error rates on all the data.
            - all these rates are averaged

        Folding:
            - partition data into n "folds"(divid)
            - train on (n-1), test on n
            - do this n times to train on all the data.
            - partitions cannont overlap

            10-Fold Cross Validation:
                - 10 is a good rule of thumb 
                - 10 times 10-fold stratifications.
                - want to make sure all classifications are represented in proportion.
                - aross partitions, class stratified in proportion
    Alternative Methods:
        Leave-Out-One:
            - train on all data points, test on one. 
            - more computationally intensive.

            note:
                - could diminish error rate predicition quality
                - could help
        
        Bootstrap:
            - finding error rates on small data sets
              (cannot stratify samples with size of folds)
            - select training (larger of two sets) usign sampling
              with replacement
            - error rates will be on the high side because training and test
              have different classification distributions
            
            - to fight this we use resubstittion to battle this
            - resub is on the lowside 

        note:
            - the resub/test set have constansts that weight each error rate prediction
            - these are apparently magical.

    Costs:
        total cost:
        - cost of doing mining and
        - benifit from the mining in the real world

        question: 
            - does benifit outway the const of the mining?

Which algo is best for problem domain:
    paired t-test:
        this is in the book, and its up to us to do this on the final project
